#!/usr/bin/env -S uv run --python=3.12 --with=fire --with=xonsh xonsh
"""AI Blob - A collection of AI tools for small tasks."""
$XONSH_SHOW_TRACEBACK = True
__import__('warnings').filterwarnings("ignore")

import fire as _fire
import platform as _platform
import subprocess
from textwrap import dedent as _dedent

_PLATFORM = _platform.system().lower()

# ///////////////
# //// UTILS ////
# ///////////////

def clip():
    ''' Return clipboard contents.
    $ blob clip
    '''
    # brew install tcl-tk
    from tkinter import Tk

    tk = Tk()
    clip = tk.clipboard_get()
    tk.destroy()
    return clip

def play(file):
    ''' Play audio file.
    $ blob play <file>
    '''
    if _PLATFORM == 'darwin':
        return subprocess.Popen(['afplay', file])
    elif _PLATFORM == 'linux':
        return subprocess.Popen(['aplay', file])
    elif _PLATFORM == 'windows':
        return subprocess.Popen(['powershell', '-c', f'(New-Object Media.SoundPlayer {file}).PlaySync()'])

def list():
    models = []
    models.extend((i[17:], '~/.cache/torch/sentence_transformers/') for i in $(ls ~/.cache/torch/sentence_transformers).split('\n'))
    models.extend((i, '~/Library/Application Support/io.datasette.llm/llama-cpp/models/') for i in $(ls "~/Library/Application Support/io.datasette.llm/llama-cpp/models").split('\n'))
    models.extend((i, '~/Library/Application Support/io.datasette.llm/gguf/models/') for i in $(ls "~/Library/Application Support/io.datasette.llm/gguf/models").split('\n'))
    models.extend((i[8:], '~/.cache/huggingface/hub/') for i in $(ls ~/.cache/huggingface/hub).split('\n'))
    models.extend((i, '~/.cache/gpt4all/') for i in $(ls ~/.cache/gpt4all).split('\n'))
    models.extend((i, '~/.cache/lm-studio/models/') for i in $(ls ~/.cache/lm-studio/models).split('\n'))
    models.extend((i.rsplit('/', 1)[-1], '~/Downloads/') for i in $(ls ~/Downloads/*.llamafile).split('\n'))

    known = 'llama phi mistral whisper qwen'.split()
    models.sort(key=lambda x: x[0].lower())
    models.sort(key=lambda x: next((n for n, i in enumerate(known) if i in x[0]), len(known)))

    for model in models:
        if not model[0]:
            continue
        print(f'{model[0]:<50}', model[1])

# ///////////////////////
# //// VISUAL MODELS ////
# ///////////////////////

def qwenvl_mlx(prompt, image):
    # brew install openblas
    # brew info openblas
    # env 
    # LDFLAGS="-L/opt/homebrew/opt/openblas/lib" 
    # CPPFLAGS="-I/opt/homebrew/opt/openblas/include" 
    # PKG_CONFIG_PATH="/opt/homebrew/opt/openblas/lib/pkgconfig" 
    uv run \
        --python=3.12 \
        --with 'numpy<2' \
        --with 'git+https://github.com/huggingface/transformers' \
        --with mlx-vlm \
        python -m mlx_vlm.generate \
            --model mlx-community/Qwen2.5-VL-7B-Instruct-8bit \
            --max-tokens 100 \
            --temp 0.0 \
            --prompt f"{prompt}" \
            --image f"{image}" # image path

def qwenvl_mlx_ocr(image):
    qwenvl_mlx("Read all the text in the image.", image)

ocr = qwenvl_mlx_ocr

def omni(url='https://github.com/microsoft/OmniParser/blob/master/requirements.txt'):
    # https://github.com/microsoft/OmniParser/blob/master/requirements.txt

    cd /tmp
    # '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome' --headless --screenshot=/tmp/shot1.png f'{url}'

    uv run \
        --with Pillow \
        --with 'git+https://github.com/microsoft/OmniParser/tree/master' \
        --no-project --with-requirements requirements.txt \
            python -c @(_dedent('''
                from util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model

                import os
                from PIL import Image

                image_path = '/tmp/shot1.png'
                image = Image.open(image_path)


                box_overlay_ratio = max(image.size) / 3200
                draw_bbox_config = {
                    'text_scale': 0.8 * box_overlay_ratio,
                    'text_thickness': max(int(2 * box_overlay_ratio), 1),
                    'text_padding': max(int(3 * box_overlay_ratio), 1),
                    'thickness': max(int(3 * box_overlay_ratio), 1),
                }
                BOX_TRESHOLD = 0.05

                import time
                start = time.time()
                ocr_bbox_rslt, is_goal_filtered = check_ocr_box(image_path, display_img = False, output_bb_format='xyxy', goal_filtering=None, easyocr_args={'paragraph': False, 'text_threshold':0.9}, use_paddleocr=True)
                text, ocr_bbox = ocr_bbox_rslt
                cur_time_ocr = time.time() 

                dino_labled_img, label_coordinates, parsed_content_list = get_som_labeled_img(image_path, som_model, BOX_TRESHOLD = BOX_TRESHOLD, output_coord_in_ratio=True, ocr_bbox=ocr_bbox,draw_bbox_config=draw_bbox_config, caption_model_processor=caption_model_processor, ocr_text=text,use_local_semantics=True, iou_threshold=0.7, scale_img=False, batch_size=128)
                cur_time_caption = time.time()

                dino_labled_img, label_coordinates, parsed_content_list
            '''))

# /////////////////////
# //// CODE SERVER ////
# /////////////////////

def llama_code_server():
    llama-server -hf ggml-org/Qwen2.5-Coder-3B-Q8_0-GGUF --port 8012 -ngl 99 -fa -ub 1024 -b 1024 --ctx-size 0 --cache-reuse 256

copilot = llama_code_server

# /////////////////
# //// YOUTUBE ////
# /////////////////

def _youtube_url_code(url: str):
    if not url.startswith("https://"):
        code = url
        url = f"https://www.youtube.com/watch?v={url}"
    else:
        code = url.split("v=")[-1]
    return url, code

def youtube_download(url: str, output: str = None):
    """download youtube video.
    $ blob yt-get <youtube id or url>
    """
    url, code = _youtube_url_code(url)
    params = []
    if output:
        params.append(f"-o {output}")

    uvx yt-dlp @(params) f"{url}"

yt_get = youtube_download

def youtube_get_audio(url: str, format: str = 'mp3'):
    """download youtube audio.
    $ blob yt-au <youtube id or url> <format=mp3>
    """
    url, code = _youtube_url_code(url)
    uvx yt-dlp -x --audio-format f'{format}' f'{url}'

yt_au = youtube_get_audio

def youtube_subtitles(url: str, /, *, lang: str = 'en'):
    '''download subtitles from youtube
    $ blob yt-sub <youtube id or url> <lang=en>
    '''
    uv pip install webvtt-py yt-dlp

    import webvtt
    from pathlib import Path

    url, code = _youtube_url_code(url)

    if not Path(f'/tmp/{code}.{lang}.vtt').exists():
        yt-dlp --skip-download --write-auto-sub --sub-lang f'{lang}' -o f'/tmp/{code}' f'{url}'

    captions = webvtt.read(f'/tmp/{code}.{lang}.vtt')
    lines = [line for caption in captions for line in caption.text.split('\n') if line.strip()] + ['']

    deduped = [l for i, l in enumerate(lines) if i == 0 or l != lines[i - 1]]

    print('\n'.join(deduped))

yt_sub = youtube_subtitles

# /////////////
# //// STT ////
# /////////////

def whisper_cpp(file):
    ''' Recognize audio
    $ blob whisper-cpp <file>
    '''
    cd /tmp
    wget --continue https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin
    ffmpeg -y -i @(file) -ar 16000 /tmp/recog.wav 2>/dev/null

    uv pip install whisper-cpp-python

    import os
    import pkgutil
    from pathlib import Path
    try:  # TODO this can be removed once the package is updated
        import whisper_cpp_python
    except FileNotFoundError:
        package = pkgutil.get_loader("whisper_cpp_python")
        whisper_path = Path(package.path)
        old = whisper_path.parent / "whisper_cpp.py"
        new = old.read_text().replace(
            'elif sys.platform == "darwin":\n        lib_ext = ".so"',
            'elif sys.platform == "darwin":\n        lib_ext = ".dylib"',
        )
        old.write_text(new)
        print("whisper_cpp.py updated")
        import whisper_cpp_python

    from whisper_cpp_python import Whisper
    whisper = Whisper(model_path="/tmp/ggml-tiny.bin")
    output = whisper.transcribe(open('recog.wav', 'rb'))
    print(output['text'])

def whisper_mlx(file):  # not tested
    ''' Recognize audio
    $ blob whisper-mlx <file>
    '''
    cd /tmp
    #ffmpeg -y -i @(file) -ar 16000 /tmp/recog.wav 2>/dev/null

    uv run --with mlx-whisper mlx_whisper f"{file}"

# /////////////
# //// TTS ////
# /////////////

def kokoro_onnx(lang="en-us", speed=1.0):
    ''' Text to speech using Kokoro model.
    $ echo 'this is a text' | blob kokoro-onnx
    '''
    # https://github.com/thewh1teagle/kokoro-onnx
    cd /tmp
    wget --continue https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx
    wget --continue https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin

    uv pip install soundfile kokoro-onnx

    import soundfile as sf
    from kokoro_onnx import Kokoro
    import subprocess
    import sys

    text = sys.stdin.read()

    kokoro = Kokoro("kokoro-v1.0.onnx", "voices-v1.0.bin")
    # af_heart af_sarah ...
    samples, sample_rate = kokoro.create(text, voice="af_heart", speed=1.0, lang=lang)
    sf.write("_.wav", samples, sample_rate)
    print("_.wav generated")
    play('_.wav')

def kokoro(speed=1):
    """
    $ echo 'this is a text' | blob kokoro 
    """
    # https://huggingface.co/hexgrad/Kokoro-82M#usage
    
    uv pip install kokoro soundfile
    cd /tmp

    from kokoro import KPipeline
    import soundfile as sf
    import sys

    text = sys.stdin.buffer.read().decode('utf8', errors="replace")

    # 'a': 'American English', 'b': 'British English', 'e': 'es', 'f': 'fr-fr', 'h': 'hi', 'i': 'it', 
    # 'p': 'pt-br', 'j': 'Japanese', 'z': 'Mandarin Chinese'
    pipeline = KPipeline(lang_code='a', repo_id='hexgrad/Kokoro-82M')

    generator = pipeline(
        text,
        voice='af_heart',  # <= change voice here
        speed=speed, split_pattern=r'\n\n\n+'
    )
    p = None
    for i, (gs, ps, audio) in enumerate(generator):
        p and p.wait()
        print(i)  # i => index
        print(gs) # gs => graphemes/text
        # print(ps) # ps => phonemes
        sf.write('_.wav', audio, 24000)
        
        p = play('_.wav')

# ///////////////////
# //// FRONTENDS ////
# ///////////////////

def openwebui():
    uvx --python 3.11 open-webui serve

# ///////////////////
# //// RECORDING ////
# ///////////////////

def record(output_file='/tmp/recording.wav', sample_rate=16000, silence_threshold=1000, silence_duration=2.0):
    '''Record audio from microphone with silence detection
    $ blob record  # Record until silence is detected
    $ blob record --silence_threshold=500  # More sensitive silence detection
    '''
    # brew install portaudio
    cd /tmp
    uv pip install pyaudio numpy

    import pyaudio, wave, sys, numpy as np, time
    
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = sample_rate
    SILENCE_THRESHOLD = silence_threshold  # Adjust this value based on your environment
    SILENCE_DURATION = silence_duration  # Silence duration in seconds before stopping
    
    p = pyaudio.PyAudio()
    
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)
    
    print("Recording... (Ctrl+C to stop)")
    
    frames = []
    silent_chunks = 0
    max_silent_chunks = int(SILENCE_DURATION * RATE / CHUNK)
    started_recording = False
    start_time = time.time()

    try:
        while True:
            data = stream.read(CHUNK)
            frames.append(data)

            audio_data = np.frombuffer(data, dtype=np.int16)
            volume = np.abs(audio_data).mean()

            # Print volume level for debugging
            bars = int(50 * volume / SILENCE_THRESHOLD) 
            bars = min(bars, 50)  # Cap at 50 characters
            print(f"Volume: {volume:.0f} {'|' * bars}{' ' * (50-bars)}\r", end="")

            # Silence detection logic
            if volume < SILENCE_THRESHOLD:
                silent_chunks += 1
                if silent_chunks > max_silent_chunks and started_recording:
                    print("\\nSilence detected, stopping recording.")
                    break
            else:
                silent_chunks = 0
                started_recording = True

    except KeyboardInterrupt:
        print("\nRecording stopped by user")

    print("Recording complete.")

    stream.stop_stream()
    stream.close()
    p.terminate()

    # Save the recorded audio
    wf = wave.open("{output_file}", 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()

    print(f"Audio saved to {output_file}")

# ///////////////////////////
# //// SEMANTIC COMMANDS ////
# ///////////////////////////

if _PLATFORM == 'darwin':
    say = read = tts = kokoro_onnx
else:
    say = read = tts = kokoro

stt = whisper_cpp

def describe(prompt, image):
    visual_model(prompt, image)

if __name__ == "__main__":
    _fire.Fire()
