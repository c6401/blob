#!/usr/bin/env -S uv run --python=3.10 --with=fire --with=xonsh xonsh
$XONSH_SHOW_TRACEBACK = True
__import__('warnings').filterwarnings("ignore", category=DeprecationWarning)

import fire as _fire
import platform as _platform
from textwrap import dedent as _dedent

_PLATFORM = _platform.system().lower()

# ///////////////
# //// UTILS ////
# ///////////////

def clip():
    ''' Return clipboard contents.
    $ blob clip
    '''
    # brew install tcl-tk
    from tkinter import Tk

    tk = Tk()
    clip = tk.clipboard_get()
    tk.destroy()
    return clip

# ///////////////////////
# //// VISUAL MODELS ////
# ///////////////////////

def qwenvl_mlx(prompt, image):
    # brew install openblas
    # brew info openblas
    # env 
    # LDFLAGS="-L/opt/homebrew/opt/openblas/lib" 
    # CPPFLAGS="-I/opt/homebrew/opt/openblas/include" 
    # PKG_CONFIG_PATH="/opt/homebrew/opt/openblas/lib/pkgconfig" 
    uv run \
        --python=3.12 \
        --with 'numpy<2' \
        --with 'git+https://github.com/huggingface/transformers' \
        --with mlx-vlm \
        python -m mlx_vlm.generate \
            --model mlx-community/Qwen2.5-VL-7B-Instruct-8bit \
            --max-tokens 100 \
            --temp 0.0 \
            --prompt f"{prompt}" \
            --image f"{image}" # image path

def qwenvl_mlx_ocr(image):
    qwenvl_mlx("Read all the text in the image.", image)

ocr = qwenvl_mlx_ocr

# ////////////////////
# //// AUTOMATION ////
# ////////////////////

def browser_use(command="Go to Reddit, search for 'browser-use', click on the first post and return the first comment."):
    # https://github.com/browser-use/browser-use
    cd /tmp

    uv run \
        --python=3.12 \
        --with browser-use \
            python -c @(_dedent(f'''
                from langchain_openai import ChatOpenAI
                from browser_use import Agent
                import asyncio
                import os

                os.system('playwright install')

                from dotenv import load_dotenv
                load_dotenv()

                async def main():
                    agent = Agent(
                        task="{command}",
                        llm=ChatOpenAI(model="gpt-4o"),
                    )
                    result = await agent.run()
                    print(result)

                asyncio.run(main())
            '''))

# /////////////////////
# //// CODE SERVER ////
# /////////////////////

def llama_code_server():
    llama-server -hf ggml-org/Qwen2.5-Coder-3B-Q8_0-GGUF --port 8012 -ngl 99 -fa -ub 1024 -b 1024 --ctx-size 0 --cache-reuse 256

copilot = llama_code_server

# /////////////////
# //// YOUTUBE ////
# /////////////////

def _youtube_url_code(url: str):
    if not url.startswith("https://"):
        code = url
        url = f"https://www.youtube.com/watch?v={url}"
    else:
        code = url.split("v=")[-1]
    return url, code

def youtube_download(url: str):
    """download youtube video.
    $ blob yt-get <youtube id or url>
    """
    url, code = _youtube_url_code(url)

    uvx yt-dlp f"{url}"

yt_get = youtube_download

def youtube_get_audio(url: str, format: str = 'mp3'):
    """download youtube audio.
    $ blob yt-au <youtube id or url> <format=mp3>
    """
    url, code = _youtube_url_code(url)
    uvx yt-dlp -x --audio-format f'{format}' f'{url}'

yt_au = youtube_get_audio

# /////////////
# //// STT ////
# /////////////

def whisper_cpp(file):
    ''' Recognize audio
    $ blob whisper-cpp <file>
    '''
    cd /tmp
    wget --continue https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin
    ffmpeg -y -i @(file) -ar 16000 /tmp/recog.wav 2>/dev/null

    uv run \
        --with whisper-cpp-python \
        --python=3.12 \
            python -c @(_dedent(f'''
                import os
                import pkgutil
                from pathlib import Path
                try:  # TODO this can be removed once the package is updated
                    import whisper_cpp_python
                except FileNotFoundError:
                    package = pkgutil.get_loader("whisper_cpp_python")
                    whisper_path = Path(package.path)
                    old = whisper_path.parent / "whisper_cpp.py"
                    new = old.read_text().replace(
                        'elif sys.platform == "darwin":\\n        lib_ext = ".so"',
                        'elif sys.platform == "darwin":\\n        lib_ext = ".dylib"',
                    )
                    old.write_text(new)
                import whisper_cpp_python
                from whisper_cpp_python import Whisper
                whisper = Whisper(model_path="/tmp/ggml-tiny.bin")
                output = whisper.transcribe(open('recog.wav', 'rb'))
                print(output['text'])
            '''))

def whisper_mlx(file):  # not tested
    ''' Recognize audio
    $ blob whisper-mlx <file>
    '''
    cd /tmp
    #ffmpeg -y -i @(file) -ar 16000 /tmp/recog.wav 2>/dev/null

    uv run --with mlx-whisper mlx_whisper f"{file}"

# /////////////
# //// TTS ////
# /////////////

def kokoro(speed=1):
    """
    $ echo 'this is a text' | blob kokoro 
    """
    # https://huggingface.co/hexgrad/Kokoro-82M#usage
    # FIXME afplay is for mac
    cd /tmp
    uv run \
        --with kokoro \
        --with soundfile \
        python -c @(_dedent(f'''
            from kokoro import KPipeline
            import soundfile as sf
            import sys
            import subprocess

            text = sys.stdin.buffer.read().decode('utf8', errors="replace")

            # 'a': 'American English', 'b': 'British English', 'e': 'es', 'f': 'fr-fr', 'h': 'hi', 'i': 'it', 
            # 'p': 'pt-br', 'j': 'Japanese', 'z': 'Mandarin Chinese'
            pipeline = KPipeline(lang_code='a')

            generator = pipeline(
                text,
                voice='af_heart',  # <= change voice here
                speed={speed}, split_pattern=r'\\n\\n\\n+'
            )
            p = None
            for i, (gs, ps, audio) in enumerate(generator):
                p and p.wait()
                print(i)  # i => index
                print(gs) # gs => graphemes/text
                # print(ps) # ps => phonemes
                sf.write(f'_.wav', audio, 24000)
                p = subprocess.Popen(['afplay', f'_.wav'])
        '''))

# ///////////////////
# //// FRONTENDS ////
# ///////////////////

def openwebui():
    uvx --python 3.11 open-webui serve


# ///////////////////////////
# //// SEMANTIC COMMANDS ////
# ///////////////////////////

say = read = tts = kokoro
stt = whisper_cpp

def describe(prompt, image):
    visual_model(prompt, image)

if __name__ == "__main__":
    _fire.Fire()
